input {
 file {
   #https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html
   #default is TAIL which assumes more data will come into the file.
   #change to mode => "read" if the file is a compelte file.  by default, the file will be removed once reading is complete -- backup your files if you need them.
   mode => "tail"
   path => "/usr/share/logstash/ingest_data/*"
 }
}


filter {
  # Parse plain text log format: 2026-02-06 04:20:00 INFO [service] message key=value
  grok {
    match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} \[%{DATA:service}\] %{GREEDYDATA:log_message}" }
  }

  # Parse key=value pairs from log_message
  kv {
    source => "log_message"
    field_split => " "
    value_split => "="
  }

  # Parse timestamp
  date {
    match => ["timestamp", "yyyy-MM-dd HH:mm:ss"]
    target => "@timestamp"
  }

  # Clean up fields
  mutate {
    remove_field => ["message", "timestamp", "log_message"]
  }

  # Add default values for common fields
  if ![username] {
    mutate { add_field => { "username" => "system" } }
  }
  if ![user_id] {
    mutate { add_field => { "user_id" => "0" } }
  }
  if ![ip] {
    mutate { add_field => { "ip" => "0.0.0.0" } }
  }

  # Convert numeric fields to proper types
  mutate {
    convert => {
      "user_id" => "integer"
      "status" => "integer"
      "duration_ms" => "integer"
      "port" => "integer"
      "usage_percent" => "integer"
      "attempt" => "integer"
      "requests" => "integer"
      "limit" => "integer"
      "amount" => "float"
      "size_mb" => "integer"
    }
  }
}


output {
 elasticsearch {
   index => "logstash-%{+YYYY.MM.dd}"
   hosts=> "${ELASTIC_HOSTS}"
   user=> "${ELASTIC_USER}"
   password=> "${ELASTIC_PASSWORD}"
   cacert=> "certs/ca/ca.crt"
 }
}